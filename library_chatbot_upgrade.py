## ì²­í‚¹ì‘ì—… ê°œì„ ë²„ì „

import os
import streamlit as st
import nest_asyncio
import re  # ğŸ‘ˆ [ì¶”ê°€] ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document  # ğŸ‘ˆ [ì¶”ê°€] Document ê°ì²´
# ğŸ‘ˆ [ì‚­ì œ] RecursiveCharacterTextSplitterëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
# from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from langchain_chroma import Chroma


# Gemini API í‚¤ ì„¤ì •
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception as e:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()

# ğŸ‘ˆ [ë³€ê²½] cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°
@st.cache_resource
def load_pdf(file_path):
    """PDFë¥¼ í˜ì´ì§€ë³„ë¡œ ë¡œë“œí•©ë‹ˆë‹¤. (split X)"""
    loader = PyPDFLoader(file_path)
    return loader.load() # ğŸ‘ˆ load_and_split() ëŒ€ì‹  load() ì‚¬ìš©

# ğŸ‘ˆ [ë³€ê²½] í•œêµ­ì–´ ëª¨ë¸ëª… ë³€ìˆ˜ë¡œ í†µì¼
KOREAN_EMBEDDING_MODEL = "jhgan/ko-sroberta-multitask"

# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    """'ì¡°' ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""

    # --- ğŸ‘‡ [ë³€ê²½] 'ì¡°' ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ëŠ” ë¡œì§ ì¶”ê°€ ---
    if not _docs:
        st.error("PDF íŒŒì¼ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
        
    # ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹©ë‹ˆë‹¤.
    full_text = "\n\n".join([doc.page_content for doc in _docs])
    source_file = _docs[0].metadata.get("source", "ë¶€ê²½ëŒ€í•™êµ ê·œì •ì§‘.pdf")

    # 'ì œ' (ê³µë°±*) ìˆ«ì (ê³µë°±*) 'ì¡°' (ê³µë°±*) (ì˜ˆ: 'ì œ1ì¡°', 'ì œ 10 ì¡°')
    article_pattern = r'(ì œ\s*\d+\s*ì¡°(?:ì˜\s*\d+)?\s*\(.+?\))' # (ê´„í˜¸ ì•ˆì˜ ì œëª© í¬í•¨)
    
    # ì •ê·œí‘œí˜„ì‹ ìˆ˜ì •: (ì œNì¡°) ë¿ë§Œ ì•„ë‹ˆë¼ (ì œNì¡°ì˜N) ë° (ì¡°í•­ ì œëª©)ê¹Œì§€ í¬í•¨
    # ì˜ˆ: ì œ1ì¡°(ëª©ì ), ì œ2ì¡°(ì ìš©ë²”ìœ„)
    # ê´„í˜¸ ì•ˆì˜ ì œëª©ì„ í¬í•¨í•˜ëŠ” ë” ê°•ë ¥í•œ ì •ê·œí‘œí˜„ì‹:
    article_pattern = r'(ì œ\s*\d+\s*ì¡°(?:ì˜\s*\d+)?\s*\(.+?\))'
    
    # í…ìŠ¤íŠ¸ë¥¼ 'ì œNì¡°(ì œëª©)' ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    articles = re.split(article_pattern, full_text)
    
    split_docs = []
    
    # ì²« ë²ˆì§¸ ì¡°ê°ì€ ë³´í†µ 'ì „ë¬¸' ë˜ëŠ” 'ëª©ì°¨'ì…ë‹ˆë‹¤.
    preamble = articles[0].strip()
    if preamble:
        split_docs.append(Document(
            page_content=preamble,
            metadata={"source": source_file, "article": "ì„œë¬¸"}
        ))

    # 'ì œ1ì¡°(ëª©ì )' ê°™ì€ ì œëª©ê³¼ ê·¸ ë‚´ìš©ì„ ë‹¤ì‹œ í•©ì¹©ë‹ˆë‹¤.
    for i in range(1, len(articles), 2):
        if i + 1 < len(articles):
            article_title = articles[i].strip() # "ì œ1ì¡°(ëª©ì )"
            article_content = articles[i+1].strip() # "â‘  ì´ ê·œì •ì€..."
            
            full_article_text = f"{article_title}\n{article_content}"
            
            # 'ì œ1ì¡°' ë¶€ë¶„ë§Œ ì¶”ì¶œ
            article_key_match = re.match(r'(ì œ\s*\d+\s*ì¡°(?:ì˜\s*\d+)?)', article_title)
            article_key = article_key_match.group(1) if article_key_match else article_title
            article_key = re.sub(r'\s+', '', article_key) # ê³µë°± ì œê±° 'ì œ1ì¡°'
            
            split_docs.append(Document(
                page_content=full_article_text,
                metadata={"source": source_file, "article": article_key}
            ))
    # --- ğŸ‘† [ë³€ê²½] íŒŒì‹± ë¡œì§ ë ---

    if not split_docs:
        st.error("PDFì—ì„œ ê·œì • 'ì¡°'í•­ì„ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    st.info(f"ğŸ“„ {len(split_docs)}ê°œì˜ ê·œì • ì¡°í•­(ì²­í¬)ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    persist_directory = "./chroma_db"
    st.info(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... ({KOREAN_EMBEDDING_MODEL})")
    embeddings = HuggingFaceEmbeddings(
        model_name=KOREAN_EMBEDDING_MODEL, # ğŸ‘ˆ [ë³€ê²½] í•œêµ­ì–´ ëª¨ë¸
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    st.info("ğŸ”¢ ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘...")
    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory
    )
    st.success("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
    return vectorstore

# ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” ChromaDBê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
@st.cache_resource
def get_vectorstore(_docs):
    persist_directory = "./chroma_db"
    
    # ğŸ‘ˆ [ë³€ê²½] ì„ë² ë”© ëª¨ë¸ì„ í•œêµ­ì–´ ëª¨ë¸ë¡œ í†µì¼
    embeddings = HuggingFaceEmbeddings(
        model_name=KOREAN_EMBEDDING_MODEL, 
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    if os.path.exists(persist_directory):
        st.info("ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì¤‘...")
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
    else:
        # _docs (í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸)ë¥¼ create_vector_storeë¡œ ì „ë‹¬
        return create_vector_store(_docs)
        
# PDF ë¬¸ì„œ ë¡œë“œ-ë²¡í„° DB ì €ì¥-ê²€ìƒ‰ê¸°-íˆìŠ¤í† ë¦¬ ëª¨ë‘ í•©ì¹œ Chain êµ¬ì¶•
@st.cache_resource
def initialize_components(selected_model):
    file_path = "[ì±—ë´‡í”„ë¡œê·¸ë¨ë°ì‹¤ìŠµ] ë¶€ê²½ëŒ€í•™êµ ê·œì •ì§‘.pdf"
    
    pages = load_pdf(file_path) # ğŸ‘ˆ [ë³€ê²½] í•¨ìˆ˜ëª…
    
    vectorstore = get_vectorstore(pages) # ğŸ‘ˆ pages ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
    retriever = vectorstore.as_retriever()

    # (ì´í•˜ í”„ë¡¬í”„íŠ¸ ë° LLM ì„¤ì •ì€ ë™ì¼)
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    qa_system_prompt = """You are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the question. \
    If you don't know the answer, just say that you don't know. \
    Keep the answer perfect. please use imogi with the answer.
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    try:
        llm = ChatGoogleGenerativeAI(
            model=selected_model,
            temperature=0.7,
            convert_system_message_to_human=False # ğŸ‘ˆ [ë³€ê²½] ê²½ê³  ì œê±°
        )
    except Exception as e:
        st.error(f"âŒ Gemini ëª¨ë¸ '{selected_model}' ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.info("ğŸ’¡ 'gemini-pro' ëª¨ë¸ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
        raise
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# Streamlit UI
st.header("êµ­ë¦½ë¶€ê²½ëŒ€ ë„ì„œê´€ ê·œì • Q&A ì±—ë´‡ ğŸ’¬ ğŸ“š")

# ğŸ‘ˆ [ì¶”ê°€] DB ì‚­ì œ ì•ˆë‚´
if os.path.exists("./chroma_db"):
    st.warning("âš ï¸ **[ì¤‘ìš”]** ì½”ë“œê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. "
               "ê¸°ì¡´ `chroma_db` ë””ë ‰í„°ë¦¬ë¥¼ **ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œ**í•œ í›„ ì•±ì„ ìƒˆë¡œê³ ì¹¨(F5)í•´ì•¼ "
               "ìƒˆë¡œìš´ ì²­í‚¹ ë°©ì‹ê³¼ ì„ë² ë”© ëª¨ë¸ì´ ì ìš©ë©ë‹ˆë‹¤.")

# ì²« ì‹¤í–‰ ì•ˆë‚´ ë©”ì‹œì§€
if not os.path.exists("./chroma_db"):
    st.info("ğŸ”„ ì²« ì‹¤í–‰ì…ë‹ˆë‹¤. ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° PDF ì²˜ë¦¬ ì¤‘... (ì•½ 3-5ë¶„ ì†Œìš”)")
    st.info("ğŸ’¡ ì´í›„ ì‹¤í–‰ì—ì„œëŠ” 10-15ì´ˆë§Œ ê±¸ë¦½ë‹ˆë‹¤!")

# Gemini ëª¨ë¸ ì„ íƒ - ìµœì‹  ëª¨ë¸ëª…ìœ¼ë¡œ ìˆ˜ì •
option = st.selectbox("Select Gemini Model",
    ("gemini-2.5-flash", "gemini-pro", "gemini-2.0-flash-exp"), # ğŸ‘ˆ [ë³€ê²½] ìµœì‹  ëª¨ë¸ëª…
    index=0,
    help="ìµœì‹  Flash ëª¨ë¸ì´ ê°€ì¥ ë¹ ë¥´ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤."
)

try:
    with st.spinner("ğŸ”§ ì±—ë´‡ ì´ˆê¸°í™” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
        rag_chain = initialize_components(option)
    st.success("âœ… ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
except Exception as e:
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.info("PDF íŒŒì¼ ê²½ë¡œì™€ API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant",
                                      "content": "êµ­ë¦½ë¶€ê²½ëŒ€ ë„ì„œê´€ ê·œì •ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!!!!!"}]

for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

if prompt_message := st.chat_input("Your question"):
    st.chat_message("human").write(prompt_message)
    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke(
                {"input": prompt_message},
                config)
            
            answer = response['answer']
            st.write(answer)
            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                for doc in response['context']:
                    # ğŸ‘ˆ [ë³€ê²½] ë©”íƒ€ë°ì´í„° 'article' í‚¤ ì°¸ì¡°
                    article_info = doc.metadata.get('article', 'N/A')
                    st.markdown(f"**ì¶œì²˜: {doc.metadata.get('source', 'N/A')} (ì¡°í•­: {article_info})**", 
                                help=doc.page_content)
